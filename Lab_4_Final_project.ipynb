{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import requests\n",
    "import json\n",
    "import pprint\n",
    "import zipfile\n",
    "import arcpy\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request Station Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding my token to a variable\n",
    "Token = \"EWfeohdvgAYBUAOZMoPgKNBjgaFAjMXS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing my params for which stations to choose\n",
    "station_params = {'datasetid': 'GHCND', 'startdate': (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d'),\n",
    "               'enddate': datetime.now().strftime('%Y-%m-%d'), 'locationid': 'FIPS:27', 'datatypeid': ['TMIN', 'TMAX'], 'limit':'1000'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the base url to a variable - Getting Stations\n",
    "base_url = 'https://www.ncdc.noaa.gov/cdo-web/api/v2/stations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the request for specific stations\n",
    "stations_req = requests.get(base_url, params=station_params, headers={'token':Token})\n",
    "stations_req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translating the request into JSON format and printing\n",
    "stations_j = stations_req.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Point Feature Class for the Available Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Wednesday, May 11, 2022 9:05:04 PM\",\"Succeeded at Wednesday, May 11, 2022 9:05:04 PM (Elapsed Time: 0.64 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'C:\\\\Users\\\\ecava\\\\Documents\\\\ArcGIS\\\\Projects\\\\Lab2\\\\Lab2.gdb\\\\stations'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating an empty feature class of type Point\n",
    "stations = stations_j['results']\n",
    "workspace = arcpy.env.workspace\n",
    "arcpy.CreateFeatureclass_management(workspace,\"stations\",\"POINT\",\"\",\"DISABLED\",\"DISABLED\",arcpy.SpatialReference(4269))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Wednesday, May 11, 2022 9:05:06 PM\",\"Adding maxdate to stations...\",\"Succeeded at Wednesday, May 11, 2022 9:05:06 PM (Elapsed Time: 0.16 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'stations'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding fields to the feature class\n",
    "arcpy.AddField_management(\"stations\",\"id\",\"TEXT\",\"\",\"\",100)\n",
    "arcpy.AddField_management(\"stations\",\"name\",\"TEXT\",\"\",\"\",100)\n",
    "arcpy.AddField_management(\"stations\",\"mindate\",\"DATE\")\n",
    "arcpy.AddField_management(\"stations\",\"maxdate\",\"DATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a cursor to add the info to the feature class\n",
    "cur = arcpy.da.InsertCursor(\"stations\", [\"id\", \"name\", \"mindate\", \"maxdate\", \"Shape@\"])\n",
    "\n",
    "for s in stations:\n",
    "   \n",
    "    point = arcpy.Point(s['longitude'],s['latitude'])\n",
    "    row = [s[\"id\"],s[\"name\"],s[\"mindate\"],s[\"maxdate\"],point]\n",
    "   \n",
    "    cur.insertRow(row)\n",
    "   \n",
    "del cur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of the stations that can be looped through to download data\n",
    "stations_list = []\n",
    "for station in stations:\n",
    "    stations_list.append(station['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for requesting temperature data\n",
    "data_req_url = 'https://www.ncdc.noaa.gov/cdo-web/api/v2/data?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Minimum Temps at Each Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary of the stations to hold Min temp data for each station\n",
    "stations_dict_min = dict.fromkeys(stations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through the list of stations \n",
    "# and making requests for the min temp for each station\n",
    "# for the time period.\n",
    "for s_name in stations_list:\n",
    "    min_params = {'datasetid': 'GHCND', 'datatypeid': 'TMIN', 'units': 'standard', 'limit': '7', 'stationid': s_name, 'startdate': (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d'), 'enddate': datetime.now().strftime('%Y-%m-%d')}\n",
    "    # make the api call\n",
    "    min_req = requests.get(data_req_url, params = min_params, headers = {'token':Token})\n",
    "    # load the api response as a json\n",
    "    min_d = min_req.json()\n",
    "    stations_dict_min[s_name] = min_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an Array of Arrays by looping through\n",
    "# The min-temp json and grabbing the date, temp, and station id and\n",
    "# Appending them to a list then appending that to the 'all-station array'\n",
    "min_all_station_array = []\n",
    "for station in stations_dict_min:\n",
    "    for results in stations_dict_min[station]['results']:\n",
    "        station_array = []\n",
    "        station_array.append(results['date'])\n",
    "        station_array.append(results['value'])\n",
    "        station_array.append(results['station'])\n",
    "        min_all_station_array.append(station_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe from the array and naming the columns\n",
    "all_min_temps = pd.DataFrame(min_all_station_array, columns = ['datetime', 'min_temp', 'station'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>datetime</th>\n",
       "      <th>2022-05-04T00:00:00</th>\n",
       "      <th>2022-05-05T00:00:00</th>\n",
       "      <th>2022-05-06T00:00:00</th>\n",
       "      <th>2022-05-07T00:00:00</th>\n",
       "      <th>2022-05-08T00:00:00</th>\n",
       "      <th>2022-05-09T00:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GHCND:CA006020559</th>\n",
       "      <td>30.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GHCND:USC00210018</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GHCND:USC00210075</th>\n",
       "      <td>37.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GHCND:USC00210287</th>\n",
       "      <td>37.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GHCND:USC00210355</th>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GHCND:USW00094960</th>\n",
       "      <td>33.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GHCND:USW00094961</th>\n",
       "      <td>33.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GHCND:USW00094963</th>\n",
       "      <td>39.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GHCND:USW00094967</th>\n",
       "      <td>29.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GHCND:USW00094992</th>\n",
       "      <td>29.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "datetime           2022-05-04T00:00:00  ...  2022-05-09T00:00:00\n",
       "station                                 ...                     \n",
       "GHCND:CA006020559                 30.0  ...                  NaN\n",
       "GHCND:USC00210018                  NaN  ...                  NaN\n",
       "GHCND:USC00210075                 37.0  ...                 50.0\n",
       "GHCND:USC00210287                 37.0  ...                  NaN\n",
       "GHCND:USC00210355                 33.0  ...                 48.0\n",
       "...                                ...  ...                  ...\n",
       "GHCND:USW00094960                 33.0  ...                  NaN\n",
       "GHCND:USW00094961                 33.0  ...                  NaN\n",
       "GHCND:USW00094963                 39.0  ...                  NaN\n",
       "GHCND:USW00094967                 29.0  ...                  NaN\n",
       "GHCND:USW00094992                 29.0  ...                  NaN\n",
       "\n",
       "[140 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pivoting the dataframe so that the index is station id and each day is a column\n",
    "min_station_data = all_min_temps.pivot(index = 'station', columns = 'datetime', values = 'min_temp')\n",
    "min_station_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Maximum Temps at Each Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary of the stations to hold data for each station\n",
    "stations_dict_max = dict.fromkeys(stations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through the list of stations and making requests for the max temp for each station\n",
    "# for the time period.\n",
    "for s_name in stations_list:\n",
    "    max_params = {'datasetid': 'GHCND', 'datatypeid': 'TMAX', 'units': 'standard', 'limit': '7', 'stationid': s_name, 'startdate': (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d'), 'enddate': datetime.now().strftime('%Y-%m-%d')}\n",
    "    # make the api call\n",
    "    max_req = requests.get(data_req_url, params = max_params, headers = {'token':Token})\n",
    "    # load the api response as a json\n",
    "    max_d = max_req.json()\n",
    "    stations_dict_max[s_name] = max_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_all_station_array = []\n",
    "for station in stations_dict_max:\n",
    "    for results in stations_dict_max[station]['results']:\n",
    "        station_array = []\n",
    "        station_array.append(results['date'])\n",
    "        station_array.append(results['value'])\n",
    "        station_array.append(results['station'])\n",
    "        max_all_station_array.append(station_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max = pd.DataFrame(max_all_station_array, columns = ['datetime', 'max_temp', 'station'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>datetime</th>\n",
       "      <th>2022-05-04T00:00:00</th>\n",
       "      <th>2022-05-05T00:00:00</th>\n",
       "      <th>2022-05-06T00:00:00</th>\n",
       "      <th>2022-05-07T00:00:00</th>\n",
       "      <th>2022-05-08T00:00:00</th>\n",
       "      <th>2022-05-09T00:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GHCND:CA006020559</th>\n",
       "      <td>63.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GHCND:USC00210018</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GHCND:USC00210075</th>\n",
       "      <td>56.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GHCND:USC00210287</th>\n",
       "      <td>62.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GHCND:USC00210355</th>\n",
       "      <td>55.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GHCND:USW00094960</th>\n",
       "      <td>65.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GHCND:USW00094961</th>\n",
       "      <td>64.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GHCND:USW00094963</th>\n",
       "      <td>64.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GHCND:USW00094967</th>\n",
       "      <td>63.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GHCND:USW00094992</th>\n",
       "      <td>42.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "datetime           2022-05-04T00:00:00  ...  2022-05-09T00:00:00\n",
       "station                                 ...                     \n",
       "GHCND:CA006020559                 63.0  ...                  NaN\n",
       "GHCND:USC00210018                  NaN  ...                  NaN\n",
       "GHCND:USC00210075                 56.0  ...                 60.0\n",
       "GHCND:USC00210287                 62.0  ...                  NaN\n",
       "GHCND:USC00210355                 55.0  ...                 59.0\n",
       "...                                ...  ...                  ...\n",
       "GHCND:USW00094960                 65.0  ...                  NaN\n",
       "GHCND:USW00094961                 64.0  ...                  NaN\n",
       "GHCND:USW00094963                 64.0  ...                  NaN\n",
       "GHCND:USW00094967                 63.0  ...                  NaN\n",
       "GHCND:USW00094992                 42.0  ...                  NaN\n",
       "\n",
       "[140 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pivoting the dataframe so that the index is station id and each day is a column\n",
    "df_max_pivot_station = df_max.pivot(index = 'station', columns = 'datetime', values = 'max_temp')\n",
    "df_max_pivot_station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Joining both min and max to one df > csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_pivot_station' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "In  \u001b[0;34m[66]\u001b[0m:\nLine \u001b[0;34m1\u001b[0m:     all_data = pd.merge(df_pivot_station, df_max_pivot_station, \u001b[33m'\u001b[39;49;00m\u001b[33mouter\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, on = \u001b[33m'\u001b[39;49;00m\u001b[33mstation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_pivot_station' is not defined\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "all_data = pd.merge(df_pivot_station, df_max_pivot_station, 'outer', on = 'station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_data.to_csv(r\"C:\\Users\\ecava\\Documents\\ArcGIS\\Projects\\Lab2\\temp_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Join\n",
    "arcpy.management.AddJoin(\"stations\", \"id\", \"temp_data.csv\", \"station\", \"KEEP_ALL\", \"NO_INDEX_JOIN_FIELDS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Arcpy describe or pd columns\n",
    "days_list = list(all_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Growing Degree Days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For corn, the degree-day accumulation base is 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2022-05-04T00:00:00', '2022-05-05T00:00:00', '2022-05-06T00:00:00', '2022-05-07T00:00:00', '2022-05-08T00:00:00', '2022-05-09T00:00:00']\n"
     ]
    }
   ],
   "source": [
    "# Creating a list of the available days to loop through\n",
    "days_list = list(min_station_data.columns)\n",
    "print(days_list)\n",
    "# a = days_list[0]\n",
    "# min_station_data[a].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of stations for use as a join column\n",
    "stations_list = df_max_pivot_station.index.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Temperature averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply take mean of each day and subtract the degree-day accumulation base (50). If the resulting number is negative, the GDD value is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = 0\n",
    "for day in days_list:\n",
    "    # Instantiating an empty dataframe\n",
    "    day_df = pd.DataFrame()\n",
    "    # Creating a stations column\n",
    "    day_df['station'] = stations_list\n",
    "    # Creating a minimum temp column\n",
    "    day_df['min'] = min_station_data[day].to_list()\n",
    "#     Creating a maximum temp column\n",
    "    day_df['max'] = df_max_pivot_station[day].to_list()\n",
    "    # Creating the mean column by adding min and max and dividing by 2\n",
    "    day_df['mean'] = day_df['min'] + day_df['max']\n",
    "    day_df['mean'] = day_df['mean'].astype(float)\n",
    "    day_df['mean'] = day_df['mean'].div(2)\n",
    "    \n",
    "    day_df['mean'] = day_df['mean'].replace(np.nan, 0)\n",
    "\n",
    "    # Calculating the GDD values for each station by subracting base value\n",
    "    day_df['gdd'] = day_df['mean'] - 50\n",
    "    \n",
    "    gdd_list = day_df['gdd'].to_list()\n",
    "#     print(gdd_list)\n",
    "    i = 0\n",
    "    while i < len(gdd_list):\n",
    "        if gdd_list[i] < 0:\n",
    "            gdd_list[i] = 0\n",
    "        i+=1\n",
    "    \n",
    "    day_df['gdd'] = gdd_list\n",
    "    \n",
    "    # Creating a save name for that day's data\n",
    "    save_name = 'gdd'+ str(a)\n",
    "    a += 1\n",
    "    # Creating a CSV from the day's data\n",
    "    day_df.to_csv(r\"C:\\Users\\ecava\\Documents\\ArcGIS\\Projects\\Lab2\\\\\" + save_name + \".csv\")\n",
    "    \n",
    "    # Joining the CSV to the station points\n",
    "    arcpy.management.AddJoin(\"stations\", \"id\", save_name+\".csv\", \"station\", \"KEEP_ALL\", \"NO_INDEX_JOIN_FIELDS\")\n",
    "    \n",
    "    # Selecting all data\n",
    "    arcpy.management.SelectLayerByAttribute(\"stations\", \"NEW_SELECTION\", \"OBJECTID IS NOT NULL\", None)\n",
    "    \n",
    "    # Exporting a copy of the stations joined to that day's data\n",
    "    arcpy.management.CopyFeatures(\"stations\", r\"C:\\Users\\ecava\\Documents\\ArcGIS\\Projects\\Lab2\\Lab2.gdb\\\\\"+ save_name, '', None, None, None)\n",
    "    \n",
    "    # Remove column of data from the stations feature class\n",
    "    arcpy.RemoveJoin_management('stations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(days_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.2727542028493769', '1.4024861317327315', '2.4471635711131228', '3.7586281798269994', '4.23678900228468', '4.670311807926968']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run interpolation on the resulting point layer\n",
    "avg_rmse_list = []\n",
    "n = 0\n",
    "while n < len(days_list):\n",
    "    n = str(n)\n",
    "    arcpy.ga.GlobalPolynomialInterpolation(\"gdd\"+n, \"gdd\"+n+\"_csv_gdd\", \"gdd\"+n+\"i\", None, 0.02130268, 1, None)\n",
    "    cvResult = arcpy.ga.CrossValidation(\"gdd\"+n+\"i\", r\"C:\\Users\\ecava\\Documents\\ArcGIS\\Projects\\Lab2\\Lab2.gdb\\gdd\"+n+\"icv\")\n",
    "    avg_rmse_list.append(cvResult.rootMeanSquare)\n",
    "    n = int(n)\n",
    "    n+=1\n",
    "avg_rmse_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Modified Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the low temperature of the day is below your crop or pests' base value, use the base temperature during your calculations.\n",
    "- If the high temperature for the day is above 86 degrees Fahrenheit, use this value instead of the actual high temperature.\n",
    "- If the mean temperature is at or below the base temperature for a crop or pest of interest, then the GDD value is zero. \n",
    "- If the mean temperature is above the base temperature, then the GDD equals the value of the mean temperature minus the base temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "modavg_rmse_list = []\n",
    "a = 0\n",
    "for day in days_list:\n",
    "    # Instantiating empty dataframes\n",
    "    day_df = pd.DataFrame()\n",
    "    min_df = pd.DataFrame()\n",
    "    max_df = pd.DataFrame()\n",
    "    \n",
    "    # Creating a stations column\n",
    "    day_df['station'] = stations_list\n",
    "    \n",
    "    # Creating a minimum temp column\n",
    "    min_df['min'] = min_station_data[day].to_list()\n",
    "    # Checking if min temp is below base temp and changing it if it is\n",
    "    min_df[min_df < 50] = 50\n",
    "    \n",
    "    # Adding new min col to main day df\n",
    "    day_df['min'] = min_df['min'].to_list()\n",
    "    \n",
    "    # Creating a maximum temp column\n",
    "    max_df['max'] = df_max_pivot_station[day].to_list()\n",
    "    # Checking if max temp is above max temp and changing it if it is\n",
    "    max_df[max_df > 86] = 86\n",
    "    # Adding new min col to main day df\n",
    "    day_df['max'] = max_df['max'].to_list()\n",
    "    \n",
    "    # Creating the mean column by adding min and max and dividing by 2\n",
    "    day_df['mean'] = day_df['min'] + day_df['max']\n",
    "    day_df['mean'] = day_df['mean'].astype(float)\n",
    "    day_df['mean'] = day_df['mean'].div(2)\n",
    "    \n",
    "    day_df['mean'] = day_df['mean'].replace(np.nan, 0)\n",
    "\n",
    "    # Calculating the GDD values for each station by subracting base value\n",
    "    day_df['gdd'] = day_df['mean'] - 50\n",
    "    \n",
    "    gdd_list = day_df['gdd'].to_list()\n",
    "    # Ensuring that if the value is negative, the value is 0 \n",
    "    i = 0\n",
    "    while i < len(gdd_list):\n",
    "        if gdd_list[i] < 0:\n",
    "            gdd_list[i] = 0\n",
    "        i+=1\n",
    "    \n",
    "    day_df['gdd'] = gdd_list\n",
    "    \n",
    "    # Creating a save name for that day's data\n",
    "    save_name = 'modgdd' + str(a)\n",
    "    a += 1\n",
    "    \n",
    "    # Creating a CSV from the day's data\n",
    "    day_df.to_csv(r\"C:\\Users\\ecava\\Documents\\ArcGIS\\Projects\\Lab2\\\\\" + save_name + \".csv\")\n",
    "    \n",
    "    # Joining the CSV to the station points\n",
    "    arcpy.management.AddJoin(\"stations\", \"id\", save_name+\".csv\", \"station\", \"KEEP_ALL\", \"NO_INDEX_JOIN_FIELDS\")\n",
    "    \n",
    "    # Selecting all data\n",
    "    arcpy.management.SelectLayerByAttribute(\"stations\", \"NEW_SELECTION\", \"OBJECTID IS NOT NULL\", None)\n",
    "    \n",
    "    # Exporting a copy of the stations joined to that day's data\n",
    "    arcpy.management.CopyFeatures(\"stations\", r\"C:\\Users\\ecava\\Documents\\ArcGIS\\Projects\\Lab2\\Lab2.gdb\\\\\"+ save_name, '', None, None, None)\n",
    "    \n",
    "    # Remove column of data from the stations feature class\n",
    "    arcpy.RemoveJoin_management('stations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2.454474466364828', '3.08435083695795', '3.2420933935500633', '3.618280860844914', '4.4458123558877745', '4.6750963656350635']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modgdd4_csv_gdd\n",
    "n = 0\n",
    "while n < len(days_list):\n",
    "    n = str(n)\n",
    "    arcpy.ga.GlobalPolynomialInterpolation(\"modgdd\"+n, \"modgdd\"+n+\"_csv_gdd\", \"modgdd\"+n+\"i\", None, 0.02130268, 1, None)\n",
    "    cvResult = arcpy.ga.CrossValidation(\"modgdd\"+n+\"i\", r\"C:\\Users\\ecava\\Documents\\ArcGIS\\Projects\\Lab2\\Lab2.gdb\\modgdd\"+n+\"icv\")\n",
    "    modavg_rmse_list.append(cvResult.rootMeanSquare)\n",
    "    n = int(n)\n",
    "    n+=1\n",
    "modavg_rmse_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Baskerville-Emin method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consists of the following steps:\n",
    "1. If max temperature is less than base temp, GDD is 0\n",
    "3. If min temp is greater than or equal to the base temp: GDD = average - base temp\n",
    "4. If 2 is not true:\n",
    "    - TAVE = ((max+min)/2)\n",
    "    - BASE = 50\n",
    "    - W = (Max - Min)/2\n",
    "    - A = Arcsin((BASE-TAVE)/W)\n",
    "    - GDD = ((W*Cos(A))-((BASE-TAVE)*(3.14/2)-A)))/3.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "be_rmse_list = []\n",
    "a = 0\n",
    "for day in days_list:\n",
    "    # Instantiating an empty dataframe\n",
    "    day_df = pd.DataFrame()\n",
    "    # Creating a stations column\n",
    "    day_df['station'] = stations_list\n",
    "    # Creating a minimum temp column\n",
    "    day_df['min'] = min_station_data[day].to_list()\n",
    "    day_df['min'] = day_df['min'].replace(np.nan, 0)\n",
    "    # Creating a maximum temp column\n",
    "    day_df['max'] = df_max_pivot_station[day].to_list()\n",
    "    day_df['max'] = day_df['max'].replace(np.nan, 0)\n",
    "    \n",
    "    gdd_list = []\n",
    "    base_temp = 50\n",
    "    for index, row in day_df.iterrows():\n",
    "        if row['max'] < base_temp:\n",
    "            gdd_list.append(0)\n",
    "        else:\n",
    "            gdd_list.append('not less than')\n",
    "            \n",
    "    nlt_indices = []\n",
    "    for i in range(len(gdd_list)):\n",
    "        if gdd_list[i] == 'not less than':\n",
    "            nlt_indices.append(i)\n",
    "    \n",
    "    for n in nlt_indices:\n",
    "        avg = ((day_df['min'][n]+ day_df['max'][n])/2)\n",
    "        if day_df['min'][n] >= 50:\n",
    "            gdd_list[n] = (avg - 50)\n",
    "        else:\n",
    "            W = ((day_df['max'][n] - day_df['min'][n])/2)\n",
    "            A = np.arcsin((base_temp-avg)/W)\n",
    "            cos = np.cos(A)\n",
    "            gdd = (W*np.cos(A))-((base_temp-avg)*(1.57)-A)/3.14\n",
    "            gdd_list[n] = gdd\n",
    "    day_df['gdd'] = gdd_list\n",
    "    day_df['gdd'] = day_df['gdd'].replace(np.nan, 0)\n",
    "    # Creating a save name for that day's data\n",
    "    save_name = 'be' + str(a)\n",
    "    a += 1\n",
    "    # Creating a CSV from the day's data\n",
    "    day_df.to_csv(r\"C:\\Users\\ecava\\Documents\\ArcGIS\\Projects\\Lab2\\\\\" + save_name + \".csv\")\n",
    "    \n",
    "    # Joining the CSV to the station points\n",
    "    arcpy.management.AddJoin(\"stations\", \"id\", save_name+\".csv\", \"station\", \"KEEP_ALL\", \"NO_INDEX_JOIN_FIELDS\")\n",
    "    \n",
    "    # Selecting all data\n",
    "    arcpy.management.SelectLayerByAttribute(\"stations\", \"NEW_SELECTION\", \"OBJECTID IS NOT NULL\", None)\n",
    "    \n",
    "    # Exporting a copy of the stations joined to that day's data\n",
    "    arcpy.management.CopyFeatures(\"stations\", r\"C:\\Users\\ecava\\Documents\\ArcGIS\\Projects\\Lab2\\Lab2.gdb\\\\\"+ save_name, '', None, None, None)\n",
    "    \n",
    "    # Remove column of data from the stations feature class\n",
    "    arcpy.RemoveJoin_management('stations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "while n < len(days_list):\n",
    "    n = str(n)\n",
    "    arcpy.ga.GlobalPolynomialInterpolation(\"be\"+n, \"be\"+n+\"_csv_gdd\", \"be\"+n+\"i\", None, 0.02130268, 1, None)\n",
    "    cvResult = arcpy.ga.CrossValidation(\"be\"+n+\"i\", r\"C:\\Users\\ecava\\Documents\\ArcGIS\\Projects\\Lab2\\Lab2.gdb\\be\"+n+\"icv\")\n",
    "    be_rmse_list.append(cvResult.rootMeanSquare)\n",
    "    n = int(n)\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare RMSE between each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the list of daily RMSE for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the three lists of rmses to see who has the lowest rmse on each day\n",
    "winner_list = []\n",
    "for r in be_rmse_list:\n",
    "    ind = be_rmse_list.index(r)\n",
    "    if r < modavg_rmse_list[ind]:\n",
    "        winner_list.append('be')\n",
    "    if r > modavg_rmse_list[ind]:\n",
    "        winner_list.append('mod')\n",
    "    n = 0\n",
    "    newind = winner_list[n]\n",
    "    if newind > avg_rmse_list[n]:\n",
    "        winner_list[n] = 'avg'\n",
    "        n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "countbe = winner_list.count('be')\n",
    "countmod = winner_list.count('mod')\n",
    "countavg = winner_list.count('avg')\n",
    "string = []\n",
    "if countbe > countmod:\n",
    "    if countbe > countavg:\n",
    "        int_string = 'be'\n",
    "if countmod > countavg:\n",
    "    int_string = 'modgdd'\n",
    "else:\n",
    "    int_string = 'gdd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modgdd\n"
     ]
    }
   ],
   "source": [
    "print(int_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push (max of 7) gdd maps to postgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically send shapefile of GDD using best model to POSTGIS database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shape@WKT', 'stations_id', 'modgdd0_csv_min', 'modgdd0_csv_max', 'modgdd0_csv_mean', 'modgdd0_csv_gdd']\n",
      "modgdd0_csv_min FLOAT,  modgdd0_csv_max FLOAT,  modgdd0_csv_mean FLOAT, modgdd0_csv_gdd FLOAT)\n",
      "['Shape@WKT', 'stations_id', 'modgdd1_csv_min', 'modgdd1_csv_max', 'modgdd1_csv_mean', 'modgdd1_csv_gdd']\n",
      "modgdd1_csv_min FLOAT,  modgdd1_csv_max FLOAT,  modgdd1_csv_mean FLOAT, modgdd1_csv_gdd FLOAT)\n",
      "['Shape@WKT', 'stations_id', 'modgdd2_csv_min', 'modgdd2_csv_max', 'modgdd2_csv_mean', 'modgdd2_csv_gdd']\n",
      "modgdd2_csv_min FLOAT,  modgdd2_csv_max FLOAT,  modgdd2_csv_mean FLOAT, modgdd2_csv_gdd FLOAT)\n",
      "['Shape@WKT', 'stations_id', 'modgdd3_csv_min', 'modgdd3_csv_max', 'modgdd3_csv_mean', 'modgdd3_csv_gdd']\n",
      "modgdd3_csv_min FLOAT,  modgdd3_csv_max FLOAT,  modgdd3_csv_mean FLOAT, modgdd3_csv_gdd FLOAT)\n",
      "['Shape@WKT', 'stations_id', 'modgdd4_csv_min', 'modgdd4_csv_max', 'modgdd4_csv_mean', 'modgdd4_csv_gdd']\n",
      "modgdd4_csv_min FLOAT,  modgdd4_csv_max FLOAT,  modgdd4_csv_mean FLOAT, modgdd4_csv_gdd FLOAT)\n",
      "['Shape@WKT', 'stations_id', 'modgdd5_csv_min', 'modgdd5_csv_max', 'modgdd5_csv_mean', 'modgdd5_csv_gdd']\n",
      "modgdd5_csv_min FLOAT,  modgdd5_csv_max FLOAT,  modgdd5_csv_mean FLOAT, modgdd5_csv_gdd FLOAT)\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "while n < len(days_list):\n",
    "    feature_class = r\"C:\\Users\\ecava\\Documents\\ArcGIS\\Projects\\Lab2\\Lab2.gdb\\\\\"+int_string+ str(n)\n",
    "    field_names = [f.name for f in arcpy.ListFields(feature_class)]\n",
    "    \n",
    "    newlist = field_names[8:]\n",
    "    fields_to_insert = ['Shape@WKT', 'stations_id']\n",
    "    fields = fields_to_insert + newlist\n",
    "    print(fields)\n",
    "\n",
    "    i = 2\n",
    "    string_add = []\n",
    "    while i < len(fields)-1:\n",
    "        string_add.append(fields[i] + ' FLOAT, ')\n",
    "        i +=1\n",
    "    string_add = ' '.join(string_add)\n",
    "    string_fields = string_add + fields[(len(fields)-1)] + ' FLOAT)'\n",
    "    print(string_fields)\n",
    "    \n",
    "    a = 1\n",
    "    end = []\n",
    "    signs = ['%s']\n",
    "    while a <len(fields)-1:\n",
    "        end.append(fields[a])\n",
    "        signs.append('%s')\n",
    "        a+=1\n",
    "\n",
    "    end_2 = ', '.join(end)\n",
    "    end_3 = end_2 + ', '+fields[(len(fields)-1)]\n",
    "\n",
    "    signs_2 = ', '.join(signs)\n",
    "    signs_3 = signs_2 + ',%s)'\n",
    "\n",
    "    work_pls = end_3 + ') VALUES ('+ signs_3\n",
    "    \n",
    "    def bulkInsert(records, table_name):\n",
    "        conn=psycopg2.connect(\"dbname='postgres' user='postgres' host='34.72.222.158' password='postgres'\")\n",
    "        cursor = conn.cursor()\n",
    "        query = \"INSERT INTO \" + table_name + ' (Shape, ' + work_pls\n",
    "        # executemany() to insert multiple rows\n",
    "        result = cursor.executemany(query, records)\n",
    "        conn.commit()\n",
    "\n",
    "    conn=psycopg2.connect(\"dbname='postgres' user='postgres' host='34.72.222.158' password='postgres'\")\n",
    "    cursor=conn.cursor()\n",
    "    cursor.execute(\"Drop Table if exists GDD_BestMethod\"+str(n))\n",
    "    string = 'CREATE TABLE GDD_BestMethod'+str(n)+' (Shape geometry, stations_id text, '\n",
    "    string_final = string + string_fields\n",
    "    \n",
    "    cursor.execute(string_final)\n",
    "    conn.commit()\n",
    "    cursor = arcpy.da.SearchCursor(feature_class, fields)\n",
    "    point_list = []\n",
    "    for row in cursor:\n",
    "        pt_tuple = ()\n",
    "        i = 0\n",
    "        while i < len(fields):\n",
    "            pt_tuple = pt_tuple + (row[i],)\n",
    "            i+=1\n",
    "        point_list.append(pt_tuple)\n",
    "    # Connect to database\n",
    "    bulkInsert(point_list, \"GDD_BestMethod\"+str(n))\n",
    "    n +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evapotranspiration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kharrufa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ET = 0.34pTa^(1.3)\n",
    "where ET is the Kharrufa potential evapotranspiration (in mm/month) Ta is mean temperature in °C, p is percentage of total daytime hours for the period used (daily or monthly) out of total daytime hours of the year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage of daytime hours and latitude: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize look-up dictionary\n",
    "pct_daylight_hrs = [['01',40,45,0.22],['02',40,45,0.24],['03',40,45,0.27], ['04',40,45,0.3], ['05',40,45,0.32],['06',40,45,0.34],['07',40,45,0.33], ['08',40,45,0.31],['09',40,45,0.28],['10',40,45,0.25],['11',40,45,0.22],['12',40,45,0.21],['01',45,50,0.2],['02',45,50,0.23],['03',45,50,0.27],['04',45,50,0.3],['05',45,50,0.34],['06',45,50,0.35],['07',45,50,0.34],['08',45,50,0.32],['09',45,50,0.28],['10',45,50,0.24],['11',45,50,0.21],['12',45,50,0.2]]\n",
    "\n",
    "# Create the pandas DataFrame\n",
    "lookup_df = pd.DataFrame(pct_daylight_hrs, columns = ['month', 'min_lat', 'max_lat', 'daylight_pct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Add New Field to stations to hold latitude\n",
    "with arcpy.EnvManager(scratchWorkspace=r\"C:\\Users\\ecava\\Documents\\ArcGIS\\Projects\\Lab2\\Lab2.gdb\", workspace=r\"C:\\Users\\ecava\\Documents\\ArcGIS\\Projects\\Lab2\\Lab2.gdb\"):\n",
    "    arcpy.management.AddField(\"stations\", \"lat\", \"FLOAT\", None, None, None, '', \"NULLABLE\", \"NON_REQUIRED\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Wednesday, May 11, 2022 10:11:28 PM\",\"Succeeded at Wednesday, May 11, 2022 10:11:46 PM (Elapsed Time: 17.83 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'stations'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Geometry of Latitude of stations\n",
    "arcpy.management.CalculateGeometryAttributes(\"stations\", \"lat POINT_Y\", '', '', None, \"SAME_AS_INPUT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Wednesday, May 11, 2022 10:12:14 PM\",\"Succeeded at Wednesday, May 11, 2022 10:12:31 PM (Elapsed Time: 17.63 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'C:\\\\Users\\\\ecava\\\\Documents\\\\ArcGIS\\\\Projects\\\\Lab2\\\\stations_TableToExcel.xlsx'>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn table into excel\n",
    "arcpy.conversion.TableToExcel(\"stations\", r\"C:\\Users\\ecava\\Documents\\ArcGIS\\Projects\\Lab2\\stations_TableToExcel.xlsx\", \"NAME\", \"CODE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Layers (Max 7 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn excel into dataframe to lookup percent\n",
    "lat_station_df = pd.read_excel(r\"C:\\Users\\ecava\\Documents\\ArcGIS\\Projects\\Lab2\\stations_TableToExcel.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_rmse_list = []\n",
    "n = 0\n",
    "for day in days_list:\n",
    "    \n",
    "    # Instantiating an empty dataframes\n",
    "    day_df = pd.DataFrame()\n",
    "    min_temp_c = pd.DataFrame()\n",
    "    max_temp_c = pd.DataFrame()\n",
    "    day_df['station'] = stations_list\n",
    "    # Creating a minimum temp column\n",
    "    # Create a min temp df from that day\n",
    "    # Convert Temps to Celsius = (°F – 32) x 5/9\n",
    "    min_orig = min_station_data[day].to_list()\n",
    "    min_temp_c['min'] = min_orig\n",
    "    min_temp_c = min_temp_c.sub(32)\n",
    "    min_temp_c = min_temp_c.div(9)\n",
    "    min_temp_c = min_temp_c.mul(5)\n",
    "    \n",
    "    # Add min column to day_df\n",
    "    day_df['min'] = min_temp_c['min'].to_list()\n",
    "    \n",
    "    # Creating a maximum temp column\n",
    "    # Create a max temp df from that day\n",
    "    # Convert Temps to Celsius\n",
    "    max_orig = df_max_pivot_station[day].to_list()\n",
    "    max_temp_c['max'] = max_orig\n",
    "    max_temp_c = max_temp_c.sub(32)\n",
    "    max_temp_c = max_temp_c.div(9)\n",
    "    max_temp_c = max_temp_c.mul(5)\n",
    "    \n",
    "    # Add max column to day_df\n",
    "    day_df['max'] = max_temp_c['max'].to_list()\n",
    "    day_df = day_df.replace(np.nan, 0)\n",
    "    \n",
    "    # Creating the mean column by adding min and max and dividing by 2\n",
    "    day_df['mean'] = day_df['min'] + day_df['max']\n",
    "    \n",
    "    # Calculating Mean column\n",
    "    mean_in_eq = pd.DataFrame()\n",
    "    mean_in_eq['mean'] = day_df['mean'].to_list()\n",
    "    mean_in_eq = mean_in_eq.div(2)\n",
    "    mean_in_eq = mean_in_eq.pow(1.3)\n",
    "    mean_in_eq = mean_in_eq.mul(0.34)\n",
    "    # Now ET = p*day_df['mean']\n",
    "    day_df['mean'] = mean_in_eq['mean'].to_list()\n",
    "    \n",
    "    # Add latitude column to day_df\n",
    "    day_df['lat'] = lat_station_df.loc[:, ['lat']]\n",
    "\n",
    "    # Instantiate empty percent daylight list\n",
    "    p_list = []\n",
    "    month = day[5:7]\n",
    "    temp_month = lookup_df.loc[lookup_df['month'] == month]\n",
    "    \n",
    "    for index, row in day_df.iterrows():\n",
    "        if row['lat'] < 45:\n",
    "            v = temp_month.index[temp_month['max_lat'] == 45].to_list()\n",
    "            v = v[0]\n",
    "            row = lookup_df.iloc[[v]]\n",
    "            pct_list = row['daylight_pct'].to_list()\n",
    "            p = pct_list[0]\n",
    "            p_list.append(p)\n",
    "        else:\n",
    "            v = temp_month.index[temp_month['max_lat'] == 50].to_list()\n",
    "            v = v[0]\n",
    "            row = lookup_df.iloc[[v]]\n",
    "            pct_list = row['daylight_pct'].to_list()\n",
    "            p =pct_list[0]\n",
    "            p_list.append(p)\n",
    "    day_df['p'] = p_list\n",
    "    day_df['et'] = day_df['p'] * day_df['mean']\n",
    "    \n",
    "    # Creating a save name for that day's data\n",
    "    save_name = 'k_et'+ str(n)\n",
    "\n",
    "    # Creating a CSV from the day's data\n",
    "    day_df.to_csv(r\"C:\\Users\\ecava\\Documents\\ArcGIS\\Projects\\Lab2\\\\\" + save_name + \".csv\")\n",
    "    \n",
    "    # Joining the CSV to the station points\n",
    "    arcpy.management.AddJoin(\"stations\", \"id\", save_name+\".csv\", \"station\", \"KEEP_ALL\", \"NO_INDEX_JOIN_FIELDS\")\n",
    "    \n",
    "    # Selecting all data\n",
    "    arcpy.management.SelectLayerByAttribute(\"stations\", \"NEW_SELECTION\", \"OBJECTID IS NOT NULL\", None)\n",
    "    \n",
    "    # Exporting a copy of the stations feature class joined to that day's data\n",
    "    arcpy.management.CopyFeatures(\"stations\", r\"C:\\Users\\ecava\\Documents\\ArcGIS\\Projects\\Lab2\\Lab2.gdb\\\\\"+ save_name, '', None, None, None)\n",
    "    \n",
    "    # Remove column of data from the stations feature class\n",
    "    arcpy.RemoveJoin_management('stations')\n",
    "\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.6524917106517513', '0.8774503247903924', '0.8909178177633358', '1.1995049885114883', '1.529571767424264', '1.7674210004015922']\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "while n < len(days_list):\n",
    "    n = str(n)\n",
    "    arcpy.ga.GlobalPolynomialInterpolation(\"k_et\"+n, \"k_et\"+n+\"_csv_et\", \"k_et\"+n+\"i\", None, 0.02130268, 1, None)\n",
    "    cvResult = arcpy.ga.CrossValidation(\"k_et\"+n+\"i\", r\"C:\\Users\\ecava\\Documents\\ArcGIS\\Projects\\Lab2\\Lab2.gdb\\k_et\"+n+\"icv\")\n",
    "    k_rmse_list.append(cvResult.rootMeanSquare)\n",
    "    n = int(n)\n",
    "    n+=1\n",
    "print(k_rmse_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blaney Criddle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ET = kp(0.46)Ta + 8.13\n",
    "\n",
    "where ET is evapotranspiration from the reference crop (in mm) for the period in which p is expressed, Ta is mean temperature in °C, p is percentage of total daytime hours for the period used (daily or monthly) out of total daytime hours of the year (365 x 12), and k is a monthly consumptive use coefficient, depending on vegetation type, location and season (using 0.8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ET = (0.8)* p * (0.46)* Ta + 8.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_rmse_list = []\n",
    "n = 0\n",
    "for day in days_list:\n",
    "    \n",
    "    # Instantiating an empty dataframes\n",
    "    day_df = pd.DataFrame()\n",
    "    min_temp_c = pd.DataFrame()\n",
    "    max_temp_c = pd.DataFrame()\n",
    "    day_df['station'] = stations_list\n",
    "    # Creating a minimum temp column\n",
    "    # Create a min temp df from that day\n",
    "    # Convert Temps to Celsius = (°F – 32) x 5/9\n",
    "    min_orig = min_station_data[day].to_list()\n",
    "    min_temp_c['min'] = min_orig\n",
    "    min_temp_c = min_temp_c.sub(32)\n",
    "    min_temp_c = min_temp_c.div(9)\n",
    "    min_temp_c = min_temp_c.mul(5)\n",
    "    \n",
    "    # Add min column to day_df\n",
    "    day_df['min'] = min_temp_c['min'].to_list()\n",
    "    \n",
    "    # Creating a maximum temp column\n",
    "    # Create a max temp df from that day\n",
    "    # Convert Temps to Celsius\n",
    "    max_orig = df_max_pivot_station[day].to_list()\n",
    "    max_temp_c['max'] = max_orig\n",
    "    max_temp_c = max_temp_c.sub(32)\n",
    "    max_temp_c = max_temp_c.div(9)\n",
    "    max_temp_c = max_temp_c.mul(5)\n",
    "    \n",
    "    # Add max column to day_df\n",
    "    day_df['max'] = max_temp_c['max'].to_list()\n",
    "    day_df = day_df.replace(np.nan, 0)\n",
    "    \n",
    "    # Creating the mean column by adding min and max and dividing by 2\n",
    "    day_df['mean'] = day_df['min'] + day_df['max']\n",
    "    \n",
    "    # Calculating Mean column\n",
    "    mean_in_eq = pd.DataFrame()\n",
    "    mean_in_eq['mean'] = day_df['mean'].to_list()\n",
    "    mean_in_eq = mean_in_eq.div(2)\n",
    "    mean_in_eq = mean_in_eq.mul(0.8)\n",
    "    mean_in_eq = mean_in_eq.mul(0.46)\n",
    "    # Now ET = p*day_df['mean']\n",
    "    day_df['mean'] = mean_in_eq['mean'].to_list()\n",
    "    \n",
    "    \n",
    "# Add latitude column to day_df\n",
    "    day_df['lat'] = lat_station_df.loc[:, ['lat']]\n",
    "\n",
    "    # Instantiate empty percent daylight list\n",
    "    p_list = []\n",
    "    month = day[5:7]\n",
    "    temp_month = lookup_df.loc[lookup_df['month'] == month]\n",
    "    \n",
    "    for index, row in day_df.iterrows():\n",
    "        if row['lat'] < 45:\n",
    "            v = temp_month.index[temp_month['max_lat'] == 45].to_list()\n",
    "            v = v[0]\n",
    "            row = lookup_df.iloc[[v]]\n",
    "            pct_list = row['daylight_pct'].to_list()\n",
    "            p = pct_list[0]\n",
    "            p_list.append(p)\n",
    "        else:\n",
    "            v = temp_month.index[temp_month['max_lat'] == 50].to_list()\n",
    "            v = v[0]\n",
    "            row = lookup_df.iloc[[v]]\n",
    "            pct_list = row['daylight_pct'].to_list()\n",
    "            p =pct_list[0]\n",
    "            p_list.append(p)\n",
    "    day_df['p'] = p_list\n",
    "    day_df['et'] = day_df['p'] * day_df['mean']\n",
    "    \n",
    "    day_df['et'] = day_df['et'] + 8.13\n",
    "    \n",
    "    # Creating a save name for that day's data\n",
    "    save_name = 'bc' + str(n)\n",
    "    \n",
    "    # Creating a CSV from the day's data\n",
    "    day_df.to_csv(r\"C:\\Users\\ecava\\Documents\\ArcGIS\\Projects\\Lab2\\\\\" + save_name + \".csv\")\n",
    "    \n",
    "    # Joining the CSV to the station points\n",
    "    arcpy.management.AddJoin(\"stations\", \"id\", save_name+\".csv\", \"station\", \"KEEP_ALL\", \"NO_INDEX_JOIN_FIELDS\")\n",
    "    \n",
    "    # Selecting all data\n",
    "    arcpy.management.SelectLayerByAttribute(\"stations\", \"NEW_SELECTION\", \"OBJECTID IS NOT NULL\", None)\n",
    "    \n",
    "    # Exporting a copy of the stations feature class joined to that day's data\n",
    "    arcpy.management.CopyFeatures(\"stations\", r\"C:\\Users\\ecava\\Documents\\ArcGIS\\Projects\\Lab2\\Lab2.gdb\\\\\"+ save_name, '', None, None, None)\n",
    "    \n",
    "        # Remove column of data from the stations feature class\n",
    "    arcpy.RemoveJoin_management('stations')\n",
    "    n +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.35285523032883814', '0.44741062496811623', '0.4257967449468412', '0.5568246157814677', '0.7224569952365121', '0.844490656206078']\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "while n < len(days_list):\n",
    "    n = str(n)\n",
    "    arcpy.ga.GlobalPolynomialInterpolation(\"bc\"+n, \"bc\"+n+\"_csv_et\", \"bc\"+n+\"i\", None, 0.02130268, 1, None)\n",
    "    cvResult = arcpy.ga.CrossValidation(\"bc\"+n+\"i\", r\"C:\\Users\\ecava\\Documents\\ArcGIS\\Projects\\Lab2\\Lab2.gdb\\bc\"+n+\"icv\")\n",
    "    bc_rmse_list.append(cvResult.rootMeanSquare)\n",
    "    n = int(n)\n",
    "    n+=1\n",
    "print(bc_rmse_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare RMSE between each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the three lists of rmses to see who has the lowest rmse on each day\n",
    "et_winner_list = []\n",
    "for r in k_rmse_list:\n",
    "    ind = k_rmse_list.index(r)\n",
    "    if r < bc_rmse_list[ind]:\n",
    "        et_winner_list.append('k')\n",
    "    else:\n",
    "        et_winner_list.append('bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "countk = et_winner_list.count('k')\n",
    "countbc = et_winner_list.count('bc')\n",
    "\n",
    "et_string = []\n",
    "if countk > countbc:\n",
    "    et_string = 'k_et'\n",
    "else:\n",
    "    et_string = 'bc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bc\n"
     ]
    }
   ],
   "source": [
    "print(et_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push (max of 7) Evapotranspiration Maps to Postgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shape@WKT', 'stations_id', 'bc0_csv_min', 'bc0_csv_max', 'bc0_csv_mean', 'bc0_csv_lat', 'bc0_csv_p', 'bc0_csv_et']\n",
      "bc0_csv_min FLOAT,  bc0_csv_max FLOAT,  bc0_csv_mean FLOAT,  bc0_csv_lat FLOAT,  bc0_csv_p FLOAT, bc0_csv_et FLOAT)\n",
      "['Shape@WKT', 'stations_id', 'bc1_csv_min', 'bc1_csv_max', 'bc1_csv_mean', 'bc1_csv_lat', 'bc1_csv_p', 'bc1_csv_et']\n",
      "bc1_csv_min FLOAT,  bc1_csv_max FLOAT,  bc1_csv_mean FLOAT,  bc1_csv_lat FLOAT,  bc1_csv_p FLOAT, bc1_csv_et FLOAT)\n",
      "['Shape@WKT', 'stations_id', 'bc2_csv_min', 'bc2_csv_max', 'bc2_csv_mean', 'bc2_csv_lat', 'bc2_csv_p', 'bc2_csv_et']\n",
      "bc2_csv_min FLOAT,  bc2_csv_max FLOAT,  bc2_csv_mean FLOAT,  bc2_csv_lat FLOAT,  bc2_csv_p FLOAT, bc2_csv_et FLOAT)\n",
      "['Shape@WKT', 'stations_id', 'bc3_csv_min', 'bc3_csv_max', 'bc3_csv_mean', 'bc3_csv_lat', 'bc3_csv_p', 'bc3_csv_et']\n",
      "bc3_csv_min FLOAT,  bc3_csv_max FLOAT,  bc3_csv_mean FLOAT,  bc3_csv_lat FLOAT,  bc3_csv_p FLOAT, bc3_csv_et FLOAT)\n",
      "['Shape@WKT', 'stations_id', 'bc4_csv_min', 'bc4_csv_max', 'bc4_csv_mean', 'bc4_csv_lat', 'bc4_csv_p', 'bc4_csv_et']\n",
      "bc4_csv_min FLOAT,  bc4_csv_max FLOAT,  bc4_csv_mean FLOAT,  bc4_csv_lat FLOAT,  bc4_csv_p FLOAT, bc4_csv_et FLOAT)\n",
      "['Shape@WKT', 'stations_id', 'bc5_csv_min', 'bc5_csv_max', 'bc5_csv_mean', 'bc5_csv_lat', 'bc5_csv_p', 'bc5_csv_et']\n",
      "bc5_csv_min FLOAT,  bc5_csv_max FLOAT,  bc5_csv_mean FLOAT,  bc5_csv_lat FLOAT,  bc5_csv_p FLOAT, bc5_csv_et FLOAT)\n"
     ]
    }
   ],
   "source": [
    "# Change the string to the winning model\n",
    "# Change the number of columns to exclude depending on the number of columns before your main data\n",
    "# Change the table name to something descriptive of the index of interest here\n",
    "n = 0\n",
    "while n < len(days_list):\n",
    "    feature_class = r\"C:\\Users\\ecava\\Documents\\ArcGIS\\Projects\\Lab2\\Lab2.gdb\\\\\"+et_string+ str(n)\n",
    "    field_names = [f.name for f in arcpy.ListFields(feature_class)]\n",
    "    \n",
    "    newlist = field_names[9:]\n",
    "    fields_to_insert = ['Shape@WKT', 'stations_id']\n",
    "    fields = fields_to_insert + newlist\n",
    "    print(fields)\n",
    "\n",
    "    i = 2\n",
    "    string_add = []\n",
    "    while i < len(fields)-1:\n",
    "        string_add.append(fields[i] + ' FLOAT, ')\n",
    "        i +=1\n",
    "    string_add = ' '.join(string_add)\n",
    "    string_fields = string_add + fields[(len(fields)-1)] + ' FLOAT)'\n",
    "    print(string_fields)\n",
    "    \n",
    "    a = 1\n",
    "    end = []\n",
    "    signs = ['%s']\n",
    "    while a <len(fields)-1:\n",
    "        end.append(fields[a])\n",
    "        signs.append('%s')\n",
    "        a+=1\n",
    "\n",
    "    end_2 = ', '.join(end)\n",
    "    end_3 = end_2 + ', '+fields[(len(fields)-1)]\n",
    "\n",
    "    signs_2 = ', '.join(signs)\n",
    "    signs_3 = signs_2 + ',%s)'\n",
    "\n",
    "    work_pls = end_3 + ') VALUES ('+ signs_3\n",
    "    \n",
    "    def bulkInsert(records, table_name):\n",
    "        conn=psycopg2.connect(\"dbname='postgres' user='postgres' host='34.72.222.158' password='postgres'\")\n",
    "        cursor = conn.cursor()\n",
    "        query = \"INSERT INTO \" + table_name + ' (Shape, ' + work_pls\n",
    "        # executemany() to insert multiple rows\n",
    "        result = cursor.executemany(query, records)\n",
    "        conn.commit()\n",
    "\n",
    "    conn=psycopg2.connect(\"dbname='postgres' user='postgres' host='34.72.222.158' password='postgres'\")\n",
    "    cursor=conn.cursor()\n",
    "    cursor.execute(\"Drop Table if exists ET_BestMethod\"+str(n))\n",
    "    string = 'CREATE TABLE ET_BestMethod'+str(n)+' (Shape geometry, stations_id text, '\n",
    "    string_final = string + string_fields\n",
    "    \n",
    "    cursor.execute(string_final)\n",
    "    conn.commit()\n",
    "    cursor = arcpy.da.SearchCursor(feature_class, fields)\n",
    "    point_list = []\n",
    "    for row in cursor:\n",
    "        pt_tuple = ()\n",
    "        i = 0\n",
    "        while i < len(fields):\n",
    "            pt_tuple = pt_tuple + (row[i],)\n",
    "            i+=1\n",
    "        point_list.append(pt_tuple)\n",
    "    # Connect to database\n",
    "    bulkInsert(point_list, \"ET_BestMethod\"+str(n))\n",
    "    n +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
